---
title: "More on Bayes Factors"
author: "Andy Wills"
output: html_document
---

```{r setup, include=FALSE}
## DEVELOPERS: Uncomment one option, as appropriate

## Show only commands.
## knitr::opts_chunk$set(echo = TRUE, message = FALSE, results='hide', fig.keep = 'none', comment=NA)

## Show commands and ouptut.
knitr::opts_chunk$set(echo = TRUE, comment=NA)

library(tidyverse)
library(BayesFactor)
cpsdata <- read_csv(url("http://www.willslab.org.uk/cps2.csv"))
cpslow <- cpsdata %>% filter(income < 150000)
```

In the [Evidence](evidence.html) worksheet, we did the following Bayesian t-test:

```{r bfttest}
ttestBF(formula = income ~ sex, data = data.frame(cpsdata))
```

Here's a more detailed explanation of the output of that test -- we'll go through each bit in turn:

`Bayes factor analysis` - You're doing a Bayes Factor analysis.

`[1] Alt., r=0.707` - In order to calculate a Bayes Factor, R has to make some assumptions about how big the difference is likely to be. The `ttestBF` command makes some broad assumptions that cover the range of effect sizes typically seen in psychology. 

More specifically, `ttestBF` assumes a Cauchy distribution of effect sizes centered on zero, with a scale of 0.707 -- this is where `r=0.707` comes from. That description probably didn't make much sense useless you have a very strong maths background, so here's the same idea, shown as a density plot. It's basically an assumption that small effect sizes are quite likely, and large effect sizes are quite unlikely.

```{r cauchy, echo=FALSE}
x<-seq(-10,10,by=0.1)
plot(x, dcauchy(x, scale = .707),type="l", xlab = "Effect size", ylab = "")
```

`18.25128` - The Bayes Factor (i.e. the main result of this analysis). This is sometimes described as the relative likelihood of a difference, compared to the absence of a difference. Under that description, the Bayes Factor reported here means it's about 18x more likely there is a difference than there isn't. But that's a bit of a simplification.

This way of describing a Bayes Factor is only accurate if, before collecting the data, you thought the presence or absence of a difference were equally likely outcomes. If that is not the case, it is more accurate to consider the Bayes Factor as the extent to which you should update your beliefs. For example, say you think telepathy is really unlikely, perhaps because there is no known mechanism by which it can operate. You might express your skepticism by saying it's about million times more likely telepathy doesn't exist than it does (a '1 in a million' chance). Say you then run an experiment on telepathy and get a Bayes Factor of 20. This tells you you should adjust your belief from 1 in a million to 1 in 50,000. So, after the experiment, you still think it's more likely telepathy doesn't exist than it does. Bayes Factors are a way of rationally updating your beliefs as new data becomes available.

`±0%` - Basically a confidence interval on the Bayes Factor. It's 0% here because we have so much data, but with smaller samples we might see something like `20 ±5%`, which would means the Bayes Factor is about 20, give or take 5%. So, it's between 19 and 21.

`Against denominator:` - This tells you that the null hypothesis is the denominator of the fraction that is used to calculate the Bayes Factor. So, in other words, you're getting BF~10~ rather than BF~01~ -- see the [Evidence](evidence.html) worksheet.

`Null, mu1-mu2 = 0` - This tells you that the null hypothesis is that the difference between the two group means (called `mu1` and `mu2` here) is zero. 

`Bayes factor type: BFindepSample` - You're doing a Bayes Factor analysis; `indepSample` is short for `independent samples`, which is another way of saying "between-subjects test". `JZS` is short for Jeffreys, Zellner, and Siow - the surnames of three people who are credited with coming up with this particular way of working out Bayes Factors.

## When p-values and Bayes Factors disagree

So, what if your p-value is less than .05, but your Bayes Factor is less than 3? The short answer is ... 
believe the Bayes Factor, and ignore the p value. The p value is there for historical reasons and doesn't tell you anything you actually want to know. 

### Long answer

Recall that the p value doesn't tell you anything that is both easy to
understand and useful to know. For example, it *doesn't* tell you the
likelihood of the null hypothesis, given your data.

Recall also that BF does give you something you want to know --- how much more
likely your experimental hypothesis is than the null hypothesis (e.g. BF = 3 means
experimental hypothesis is 3x more likely)

Now, take a look at [Wetzels et al. (2011)](http://pcl.missouri.edu/sites/default/files/Wetzels:etal:2011.pdf). In situations where .01 < p < .05,
most of the time Bayesian analysis indicates the evidence is
anecdotal (i.e. BF < 3). If you were going to stick to p values, you wouldn't
pick the p <.05 threshold -- it's just far too lenient.

Then, take a look at [Masicampo & Lalande (2012)](https://www.researchgate.net/profile/Daniel_Lalande/publication/230594320_A_Peculiar_Prevalence_of_p_Values_Just_Below_05/links/55f97a1408aeafc8ac259527/A-Peculiar-Prevalence-of-p-Values-Just-Below-05.pdf?origin=publication_detail) ... p values in Psychology are
very  commonly in the .01 < p <.05 region.

Put that all together and we begin to get a sense of one of the causes of the
replication crisis, and how BF may help us escape it.

___


#### Licence
This material is distributed under a [Creative Commons](https://creativecommons.org/) licence. CC-BY-SA 4.0. 

___

_version: 0.2_
