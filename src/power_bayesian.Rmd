---
author: Lenard Dome
title: Estimating Sample Size for Bayes Factor
subtitle: Estimate sample size for planned analysis with Bayes Factor
output:
  html_document:
    toc: true
    toc_depth: 2
date: '`r format(Sys.Date())`'
bibliography: 'power_bayesian.bib'
---



# Before you start

This is an advanced worksheet, which assumes you have completed the [Absolute Beginners' Guide to R](https://ajwills72.github.io/rminr/#beginners), the [Research Methods in Practice (Quantitative section)](https://benwhalley.github.io/rmip/overview-quantitative.html), and the [Intermediate Guide to R](https://ajwills72.github.io/rminr/#rmip).

If your memory is a bit fuzzy, have a look at these resources:

- [Evidence pt.1 Bayesian and traditional t-tests](https://ajwills72.github.io/rminr/vbg_evidence.html)
- [Evidence pt.2 Bayes and traditional correlation, scatterplot](https://ajwills72.github.io/rminr/vbg_corr.html)
- [Group Differences](https://www.andywills.info/rminr/group-differences.html)
- [Statistical Power](https://www.andywills.info/rminr/power.html)
- [More on Statistical Power](https://www.andywills.info/rminr/effsize_from_papers.html)

# Power analysis with Bayes Factors

In traditional null hypothesis significance testing, power analysis estimates
the likelihood
of rejecting the null hypothesis given the specific effect size of the
alternative hypothesis. So if you have two groups, you could use a point value
like that of Cohen's D to find out how many data points you need to get a
significant result when comparing the two groups. This is useful, because it
explicitly states how likely to achieve the researchers goal in the anticipated
experiment [@kruschke2013bayesian]. Ideally, you also want to do it
prospectively rather than retrospectively - before data collection and not
after. In order to estimate a sample size and power, we need to think about
both our experiment and our planned analysis.

There is a good chance that someone has done an experiment similar to what you
are planning. So you can use their data to find the effect size and estimate
something about your experiment - usually sample size. Using your own data to
post-hoc estimate power, after the analysis has been carried out, adds nothing
that a p-value wouldn't tell you. Post-hoc power is not useful for making
inferences from/about the data that you have. It also doesn't allow you to
plan for the event when the null hypothesis is true. This is because p-values
don't let you conclude from null results.

In Bayesian Power Analysis, we are looking at it from a different
angle. Here, we want to estimate precision - the probability of a value
falling within a certain interval. This tells us how likely that we will
have conclusive results given some details of your experiment
(i.e. sample size). We calculate it for Bayes Factors as

$$1 - Pr(1/10 < BF < 10)$$

We are looking at intervals, because
usually we are interested in an entire distribution of parameters. The Bayes
Factor is one such case where we are not as interested in intervals, but
rather it is used as the traditional p-value. The main difference
is that you can conclude from null results, which makes it an attractive
metric. So there are two big groups of Bayes Factors: conclusive vs inconclusive.

**When we think about Bayes Factors and Power, we want to know how
many data points we need to get a conclusive Bayes Factor.**

## The Algorithm

> An algorithm is a list of rules to follow in order to solve a problem.

This is the algorithm as described by [@kruschke2013bayesian; @kruschke2014doing]
for Bayesian Parameter estimation. @schonbrodt2018bayes adapts this algorithm
to what they call **fixed-n designs**, where you test a set number of participants
and calculate a Bayes Factor. @schonbrodt2018bayes also skips some steps, and
decide to group it under an umbrella term: **Bayes Factor Design Analysis**.
I just call it estimating sample size.

We will use @schonbrodt2018bayes algorithm, because it is the simplest form and
is suitable for most psychology experiments.

The steps:

1. Generate a large number (e.g. 2000) of random data points by taking some assumptions about what to expect. Data should be generated according to how the actual data will be collected in the planned experiment. For example, there will be a fixed number of participants, fixed number of trials, fixed number of conditions. You also want to take some other assumptions, like what is the effect size you might expect.
2. Subset the data to include a fixed number of data points (40 participant). You ideally want to do it for different sample sizes for the same data set.
3. Run a test on the subset and tally whether the test is precise enough. You simply run a Bayes Factor t-test in the subsets and check if the Bayes Factor is between 1/3 and 3.
4. Repeat steps 2 and 3 many thousands of times, for example 10000.
5. Pick a well-powered sample size.

## Walk-through Bayesian Power Analysis

Let us say that we are interested in multistable perception and want to do
a replication of @mcdougall1921effects. They did not include the data, but
told us some details about their analysis, including some summary statistics.

@mcdougall1921effects were interested in how different substances (coffee,
antidepressant) affects the perception of specific type of visual illusions

Here we will only focus on the condition where participants viewed an
an ambiguous figure called the
[Necker Cube](https://en.wikipedia.org/wiki/Necker_cube). A necker cube
is the frame of the cube with no visual cues on its orientation, which
makes it ambiguous. It is ambiguous, because while viewing the figure
you can interpret it to have either the lower-left or the upper-right square
at the front. If you keep looking at the cube, you can see that the orientiation
switches between these two percepts - having the lower-left or upper-right
square as its front side.

![Necker Cube](https://upload.wikimedia.org/wikipedia/commons/5/52/Necker_cube_and_impossible_cube.svg)

What @mcdougall1921effects found that peple switch between the two percepts
much more often after they had coffee compared to when they hadn't had
any other drug like antidepressant - which makes people experience less
of these switches.

### Starting with the data

```{r}
# always start with loading all the packages you need for a given analysis
library(BayesFactor, quietly = TRUE)
library(effsize)
library(tidyverse)

# set seed for reproducability
set.seed(1)
```

In most scenarios, you will start from the next step, but your supervisor might
have some data you can use, but nowadays people share their data on places, like
[OSF](https://osf.io/). Here, I just made up the data according to some really
vague description by the original authors. Let us start by importing said data.

You can download the data from this [link](https://raw.githubusercontent.com/ajwills72/rminr/power-bayes/src/data/mcdougall1921madeup.csv).

```{r}
# import dataset
dta <- read_csv("data/mcdougall1921madeup.csv")
```

Let us have a look at how the data look like.

```{r}
dta %>%
    group_by(condition) %>%
    summarise(mean = mean(fluctuations), sd = sd(fluctuations))
```

The means are indeed promising, but we have long learned not to base
our conclusion solely on the mean, so we go on and do the analysis we hope to
do on our own data.

```{r}
bf <- ttestBF(x = dta$fluctuations[dta$condition == "before"],
              y = dta$fluctuations[dta$condition == "after"], paired = TRUE)
bf
```

Now we see that it is an inconclusive result. Some would say that it is
anecdotal evidence for the difference, but there just isn't enough data to
say that.

```{r}
cohen.d(fluctuations ~ condition | Subject(ppt), data = dta, paired = TRUE)
```

Our results are not conclusive, as BF = `r data.frame(bf)$bf`. So we want to
know how many participants we need to recruit so that we can conclude
from this experiment.

## Generating data

The next step is to come up with some generated data. If you don't have access
to any real data to begin with, you should start with this step. We know that
we have a small effect size with Cohen's $d = 0.217$ and that our inconclsive
BF shifts towards the alternative hypothesis - that there is a difference.

Based on these, we can quantitatively express our belief about this experiment.
In order to do that, we have to get two sets of data in the form of two posteriors:
one for the control condition and one for the experimental one.

Let us start with the **control condition**. We will use a command called
`rbinom`.

The first argument `n` will be the number of observation we
make (how many participants we want overall). We will set it to 1000,
because we will randomly pick a smaller subset from this 1000 when we
start estimating the sample size.

The second argument will be `size`,
which specifies the maximum number of fluctuation a person can experience
in a given time. We will set it to 100. We assume that participant will
fixate on the Necker Cube for 100 seconds and they can't have more than one
in a second.

The third argument `prob` will determine the mean proportion of fluctuations
relative to fluctuating every second. This is that we will vary between conditions.
there will be a 3% increase in the experimental conditions, which will result
in a small effect size.

We will do the same for the experimental condition, but the mean proportion
will be larger. We pick the difference to reflect the effect size we saw in the
real data. Alternatively, if you have no data you can just use an effect size
you think is reasonable.

```{r}
control <- rbinom(1000, 100, 0.60)
experimental <- rbinom(1000, 100, 0.63)
group <- rep(c("before", "after"), each = 1000)
participants <- rep(1:1000, 2)
ideal_data <- tibble(ppt = participants,
                     condition = group,
                     fluctuations = c(control, experimental))
```


```{r}
ideal_data %>% ggplot(aes(x = fluctuations, fill = condition)) +
    geom_density(alpha = 0.5) +
    theme_void()
```

## Writing a function to get the Bayes Factor

Every function (commands) that you use has been written by a person. Functions
as you have noticed, take something as an input, do something to that input,
and return the result of what they have done to the input. There is some good
material already on [function](https://www.andywills.info/rminr/awdiss.html#funcloop)
by Paul Sharpe and Andy Wills. To put it briefly, a **function is an
instruction sequence that takes something as an input and translates it
into an output.**

Here we will write our own function that extracts the Bayes Factor for our
sample size. The function will need to know `n` which is our sample size,
and `data` which is the generated data.
So what does our function need to do? It needs to :

1. Take `n` number of randomly selected participant from `data`.
2. Then run the paired-sample Bayesian t-test on the subset data.
3. Store the output and put it into a format, where we can extract the Bayes Factor.
4. At the end, return the Bayes Factor.

```{r}
get_bf <- function(n, data) {
    ## 1. we randomly pick n number of participants
    ideal_dta <- data %>% filter(ppt %in% sample(1000, n))
    ## 2. do the Bayesian t-test
    test <- ttestBF(x = ideal_dta$fluctuations[ideal_dta$condition == "before"],
                  y = ideal_dta$fluctuations[ideal_dta$condition == "after"],
                  paired = TRUE)
    ## 3. get the Bayes Factor
    ## we have to change the data format to extract the Bayes Factor
    test <- data.frame(test)
    ## copy it into a new R object
    bf <-  test$bf
    ## 4. return it to the user
    return(bf)
}
```

## Determining Sample Size

Now we are at the stage when we can actually run a simulation. These simualtions
will be based on a mathematical technique called random sampling or Monte Carlo
methods. Beware that there are no special equations or complicated math
involved here. Monte Carlo is simply a tool that is used to estimate possible
outcomes of uncertain events. In our case, the possible outcome is the Bayes
factor and the uncertain event is the data we collect.

This will take a while.

```{r}
monte <- tibble(n = rep(seq(10, 200, 10), each = 1000)) %>%
    mutate(bf = map_dbl(n, get_bf, data = ideal_data))
```

Now we have everything that we need. So let us do the math.

1. We will group by the sample sizes.

2. Count how many Bayes Factor fell between 1/3 and 3. We do this with the
command `between`, which will return TRUE if the value of interest falls
between the lower and upper bounds of the interval. So it will look like
`between(value, lower, upper)`. For example, `between(30, 18, 60)`, would
return TRUE, because 30 is somewhere between 18 and 60.

3. We then `sum` it up, which will count the number of times it returns TRUE.
Divide that number by a 100 to get the proportions.

4. Then we move on to subtract it from 1, which will give us the proportion of
times we got a Bayes Factor that was conclusive - outside of the interval
1/3 and 1.

```{r}
monte %>%
    group_by(n) %>%
    summarise(power = 1 - (sum(between(bf, 1/3, 3)) / 1000))
```

```{r}
monte %>%
    group_by(n) %>%
    summarise(power = 1 - (sum(between(bf, 1/3, 3)) / 1000)) %>%
    ggplot(aes(x = n, y = power)) +
    geom_line() +
    geom_point() +
    xlab("sample size") +
    ylab("power") +
    theme_classic()
```

## Exercise and Assessment

**This is a Psyc:EL task.**

In this exercise you will need to estimate the adequate sample size for the
null effect. Additionally, you will need to do so for a harsher interval,
where the Bayes Factor falls outside of 1/10 and 10. You will need
to find the sample size, that will give you at least 90% of power.

You will need to report that sample size in Psych:EL. You will also need to upload the code with comments for:

1. The `function` that extracts the Bayes Factor.
2. The code you wrote for the simulation.
3. The code you wrote for calculating the power.

You will also need to create two plots and **upload it to Psych:EL as a PDF**:

1. A plot showing the distribution of the data you generated - `geom_density()`
2. A plot showing the power for each sample size - `geom_line()`

# Further Reading

- [How would you explain Markov Chain Monte Carlo (MCMC) to a layperson?](https://stats.stackexchange.com/a/207)
- [MIT OpenCourseWare: Monte Carlo Simulation](https://www.youtube.com/watch?v=OgO1gpXSUzU)
- [Sch√∂nbrodt, F. D. & Wagenmakers, E.-J. (2018). Bayes Factor Design Analysis: Planning for compelling evidence. Psychonomic Bulletin & Review, 25, 128-142. doi:10.3758/s13423-017-1230-y](https://osf.io/d4dcu)

Here is a list of papers already using Bayesian Power Analysis:

- [BFDA in practice: A list of published examples](https://github.com/nicebread/BFDA/blob/master/BFDA_examples.md)

## REFERENCES
