---
author: Lenard Dome
title: Estimating Sample Size for Bayes Factor
subtitle: Estimate sample size for planned analysis with Bayes Factor
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
date: '`r format(Sys.Date())`'
bibliography: 'power_bayesian.bib'
---

# Before you start

This is an advanced worksheet, which assumes you have completed the [Absolute Beginners' Guide to R](https://ajwills72.github.io/rminr/#beginners), the [Research Methods in Practice (Quantitative section)](https://benwhalley.github.io/rmip/overview-quantitative.html), and the [Intermediate Guide to R](https://ajwills72.github.io/rminr/#rmip).

If your memory is a bit fuzzy, have a look at these resources:

- [Evidence pt.1 Bayesian and traditional t-tests](https://ajwills72.github.io/rminr/vbg_evidence.html)
- [Evidence pt.2 Bayes and traditional correlation, scatterplot](https://ajwills72.github.io/rminr/vbg_corr.html)
- [Group Differences](https://www.andywills.info/rminr/group-differences.html)
- [Statistical Power](https://www.andywills.info/rminr/power.html)
- [More on Statistical Power](https://www.andywills.info/rminr/effsize_from_papers.html)

# Power analysis with Bayes Factors

In traditional null hypothesis significance testing, power analysis estimates
the likelihood
of rejecting the null hypothesis given the specific effect size of the
alternative hypothesis. So if you have two groups, you could use a point value,
like that of [Cohen's *d*](https://www.andywills.info/rminr/power.html), to
find out how many data points you need to get a
significant result when comparing the two groups. This is useful, because it
explicitly states how likely to achieve the researchers goal in the anticipated
experiment [@kruschke2013bayesian]. Ideally, you also want to do it
prospectively rather than retrospectively - before data collection and not
after. In order to estimate a sample size and power, we need to think about
both our experimen, our planned analysis and what others have already done.

There is a good chance that someone has done an experiment similar to what you
are planning. So you can use their data to find the effect size and estimate
something about your experiment - usually sample size. Using your own data to
post-hoc estimate power, after the analysis has been carried out, adds nothing
that a p-value wouldn't tell you. Post-hoc power is not useful for making
inferences from/about the data that you have. It also doesn't allow you to
plan for the event when the null hypothesis is true. This is because p-values
don't let you conclude from null results.

In Bayesian Power Analysis, we are looking at it from a different
angle. Here, we want to estimate precision - the probability of a value
falling within a certain interval. This tells us how likely that we will
have conclusive results given some details of your experiment
(i.e. sample size). We calculate it for the Bayes Factor (BF) as

$$1 - Pr(lower bound < BF < upper bound)$$

This might be a bit confusing, because now we will be looking at intervals
and not thresholds, because we are interested in an entire distribution of
values. In psychology, people like to think categorically, therefore define
categories of BFs depending on different intervals. In reality,
BF is a continuous value, that gives you an estimate on how likely
is it that your belief describes the data compared to some other belief. For
estimating the sample size, we can define two big groups of Bayes Factors:
conclusive vs inconclusive. Inconclusive BF can be any BF that falls between
the lower bound and the upper bound. All other BFs are conclusive.

**When we think about Bayes Factors and Power, we want to know how
many data points we need to get a conclusive Bayes Factor.**

## The Algorithm

> An algorithm is a list of rules to follow in order to solve a problem.

The algorithm for estimating sample size for more sophisticated Bayesian
analysis was described by [@kruschke2013bayesian; @kruschke2014doing].
@schonbrodt2018bayes adapts this algorithm to **fixed-n designs** and
to the already familiar **Bayes Factor**. In a fixed-n design, you test
a set number of participants and calculate something like a Bayes Factor
at the end. @schonbrodt2018bayes decide to group this technique under an
umbrella term: **Bayes Factor Design Analysis**. I just call it estimating
sample size.

We will use @schonbrodt2018bayes algorithm, because it is the simplest form and
is suitable for most psychology experiments you will be doing in your final
year project.

Here I will show you the algorithm and then walk you through it. The steps
in the algorithm:

1. Generate a large number (e.g. 2000) of random data points by taking some assumptions about what to expect. Data should be generated according to how the actual data will be collected in the planned experiment. For example, there will be a fixed number of participants, fixed number of trials, fixed number of conditions. You also want to take some other assumptions, like what is the effect size you might expect.
2. Subset the data to include a fixed number of data points (40 participant). You ideally want to do it for different sample sizes for the same data set.
3. Run a test on the subset and tally whether the test is precise enough. You simply run a Bayes Factor t-test in the subsets and check if the Bayes Factor is between 1/3 and 3.
4. Repeat steps 2 and 3 many thousands of times, for example 10000.
5. Pick a well-powered sample size.

## Walk-through Bayesian Power Analysis

Let us say that we are interested in multistable perception and want to do
a replication of @mcdougall1921effects. They did not include the data, but
told us some details about their analysis, including some summary statistics.

@mcdougall1921effects were interested in how different substances (coffee,
antidepressant) affects the perception of a subset of visual illusions.
Here we will only focus on the condition where participants viewed an
an ambiguous figure called the
[Necker Cube](https://en.wikipedia.org/wiki/Necker_cube). A necker cube
is the frame of the cube with no visual cues on its orientation, which
makes it ambiguous. It is ambiguous, because while viewing the figure
you can interpret it to have either the lower-left or the upper-right square
at the front. If you keep looking at the cube, you can see that the orientiation
switches between these two percepts - having the lower-left or upper-right
square as its front side.

![Necker Cube](https://upload.wikimedia.org/wikipedia/commons/5/52/Necker_cube_and_impossible_cube.svg)

@mcdougall1921effects reported that people passively switch
between the two percepts
at a higher rate after they had coffee compared to when they hadn't had
any other drug.

### Starting with the data

```{r, include = FALSE}
# always start with loading all the packages you need for a given analysis
library(BayesFactor, quietly = TRUE)
library(effsize)
library(tidyverse)

# set seed for reproducability
set.seed(1)
```

```r
# always start with loading all the packages you need for a given analysis
library(BayesFactor, quietly = TRUE)
library(effsize)
library(tidyverse)

# set seed for reproducability
set.seed(1)
```

In most scenarios, you will start from the next step, but your supervisor might
have some data you can use. Nowadays people share their data on places like
[OSF](https://osf.io/) or [GitHub](https://github.com/). Here, I just made up
the data according to some really vague description by the original authors.
Let us start by importing said data.

You can download the data from this [link](https://raw.githubusercontent.com/ajwills72/rminr/power-bayes/src/data/mcdougall1921madeup.csv).

```{r}
# import dataset
dta <- read_csv("data/mcdougall1921madeup.csv")
```

Let us have a look at how the data look like.

```{r}
dta %>%
    group_by(condition) %>%
    summarise(mean = mean(fluctuations), sd = sd(fluctuations))
```

The means are indeed promising, but we have long learned not to base
our conclusion solely on the mean, so we go on and do the analysis we plan to
do on our own data.

```{r}
bf <- ttestBF(x = dta$fluctuations[dta$condition == "before"],
              y = dta$fluctuations[dta$condition == "after"], paired = TRUE)
bf
```

This is an inconclusive result. Some would say that it is
anecdotal evidence for the difference, but that is a bit of a cop-out.
We just don't have enough data to conclude.

Our results are not conclusive, as BF = `r data.frame(bf)$bf`. The next step
is then to figure out how many participants we need to recruit so that our
analysis can conclude. First, let us look at the effect size.

```{r}
cohen.d(fluctuations ~ condition | Subject(ppt), data = dta, paired = TRUE)
```

## Generating data

The next step is to come up with some generated data. If you don't have access
to any real data to begin with, you should start with this step. We know that
we have a small effect size with Cohen's $d = 0.217$ and that our inconclsive
BF shifts towards the alternative hypothesis - that there is a difference.
If the effect size is not reported by the authors, you can still work it out.
More information on how to do it can be found in the
[more on power worksheet](https://www.andywills.info/rminr/effsize_from_papers.html).

Based on these, we can quantitatively express what we expect to happen in
our experiment.
We do this by generating some data that will look exactly like the one
that we will collect. The first thing to jot down is to decide how our
dependent variable will look like.

We know that we will have count data, because
our dependent variable is the count of how many switches participants made
between having the lower-left or upper-right square as the cube's front side.
`rbinom` is the command that is most suitable to generating this type of
data. `rbinom` looks like this:

```r
rbinom(n, size, prob)
```

The first argument `n` will be the number of observation we
make (how many participants we want overall). We will set it to 1000,
because we will randomly pick a smaller subset from this 1000 when we
start estimating the sample size.

The second argument will be `size`,
which specifies the maximum number of fluctuation a person can experience
in a given time. We will set it to 100. We assume that participant will
fixate on the Necker Cube for 100 seconds and they can't have more than one
in a second. This is less important here, because `n` will scale up to
whatever `size` we set. Note that `size` has to be same for both conditions.

The third argument `prob` will determine the mean proportion of fluctuations
relative to fluctuating every second. This is that we will vary between conditions.
there will be a 3% increase in the experimental conditions, which will result
in a small effect size.

We will do the same for the experimental condition, but the mean proportion
will be larger. We pick the difference to reflect the effect size we saw in the
real data. Alternatively, if you have no data you can just use an effect size
you think is reasonable.

```{r}
# each data point is a single participant
control <- rbinom(1000, 100, 0.60)
experimental <- rbinom(1000, 100, 0.63)
# here we create a vector for the group column
group <- rep(c("before", "after"), each = 1000)
# this object will have the participant unique IDs
participants <- rep(1:1000, 2)
# here we combine everything together
ideal_data <- tibble(ppt = participants,
                     condition = group,
                     fluctuations = c(control, experimental))
```

Now we check whether it looks okay. First, we should have three columns.
`ppt` should include the participant ID, `condition` should include
whether it is before or after caffeine intake, and `fluctuations`
should be the number of fluctuations people reported.

The distribution of the data also looks exactly what we want.

```{r}
ideal_data %>% ggplot(aes(x = fluctuations, fill = condition)) +
    geom_density(alpha = 0.5) +
    theme_void()
```

## Writing a function to get the Bayes Factor

Every function (commands) that you use has been written by a person. Functions
as you have noticed, take something as an input, do something to that input,
and return the result of what they have done to the input. There is some good
material already on [function](https://www.andywills.info/rminr/awdiss.html#funcloop)
by Paul Sharpe and Andy Wills. To put it briefly, a **function is an
instruction sequence that takes something as an input and translates it
into an output.**

Here we will write our own function that extracts the Bayes Factor for our
sample size. The function will need to know `n` which is our sample size,
and `data` which is the generated data.
So what does our function need to do? It needs to :

1. Take `n` number of randomly selected participant from `data`.
2. Then run the paired-sample Bayesian t-test on the subset data.
3. Store the output and put it into a format, where we can extract the Bayes Factor.
4. At the end, return the Bayes Factor.

```{r}
## we take the sample size n and data as inputs
get_bf <- function(n, data) {
    ## 1. we randomly pick n number of participants from data
    ideal_dta <- data %>% filter(ppt %in% sample(1000, n))
    ## 2. do the Bayesian t-test
    test <- ttestBF(x = ideal_dta$fluctuations[ideal_dta$condition == "before"],
                  y = ideal_dta$fluctuations[ideal_dta$condition == "after"],
                  paired = TRUE)
    ## 3. get the Bayes Factor
    ## we have to change the data format to extract the Bayes Factor
    test <- data.frame(test)
    ## copy it into a new R object
    bf <-  test$bf
    ## 4. return it to the user
    return(bf)
}
```

## Determining Sample Size

> Simulations are pretend games. When we say simulate, we mean to imitate some
real-world process over time.

Now we are at the stage when we can actually run a simulation. These simualtions
will be based on a mathematical technique called random sampling or Monte Carlo
methods. Beware that there are no special equations or complicated math
involved here. Monte Carlo is simply a tool that is used to estimate possible
outcomes of uncertain events. In our case, the possible outcome is the Bayes
factor and the uncertain event is the data we collect. So we imitate
data collection by generating some random data, check the Bayes Factor for
the data, and repeat it many times to get a robust and more accurate result.

We have to repeat it many times, because the process of sampling *n* number
of data points is random.

There are two things to decide before we start imitating data collection:

1. The sample sizes we want to check.
2. The number of draws we wish to

This will take a while.

```{r}
monte <- tibble(n = rep(seq(10, 200, 10), each = 1000)) %>%
    mutate(bf = map_dbl(n, get_bf, data = ideal_data))
```

Now we have everything that we need. So let us do the math.

1. We will group by the sample sizes.

2. Check whether the BF fell between 1/3 and 3. We do this with the
command `between`, which will return TRUE if the value of interest falls
between the lower and upper bounds of the interval. So it will look like
`between(value, lower, upper)`. For example, `between(30, 18, 60)`, would
return TRUE, because 30 is somewhere between 18 and 60.

3. We then `sum` it up, which will count the number of times it returns TRUE.
Divide that number by a 100 to get the proportions.

4. Then we move on to subtract it from 1, which will give us the proportion of
times we got a Bayes Factor that was conclusive - outside of the interval
1/3 and 1.

```{r}
monte %>%
    group_by(n) %>%
    summarise(power = 1 - (sum(between(bf, 1/3, 3)) / 1000))
```

The only thing that remains is to find out the actual sample size we need.
In frequentist methods, people usually expect your study to have at least
80% power. If we accept that convention for this as well, **we can settle on
a sample size of 50.** On the other hand, the story doesn't end here.

This is the exact moment, you might want to stop and think about how
many participants you might potentially end up excluding. This depends a lot
on the nature of your experiment and the characteristics of the population.
If you have learning criterion, you can end up excluding 30% of all
participants. This means that it is not uncomon to add 15 more participants
on top of that 50, just to be safe.

BV

As you can see, the power goes up not linearly, but more as
a curve. This resembles more to a logarithmic function, which has this
steep curve that seems to plateau after a certain point. Power after a certain
point remains stationary. This means that collecting too much data is
not harmful, but can be unnecessary.

```{r}
monte %>%
    group_by(n) %>%
    summarise(power = 1 - (sum(between(bf, 1/3, 3)) / 1000)) %>%
    ggplot(aes(x = n, y = power)) +
    geom_line() +
    geom_point() +
    xlab("sample size") +
    ylab("power") +
    theme_classic()
```

# Exercise and Assessment

**This is a Psyc:EL task.**

In this exercise you will need to estimate the sample size for detecting
null effect. Additionally, you will need to do so for a harsher interval,
where the Bayes Factor falls outside of 1/10 and 10. You will need
to find the sample size, that will give you at least 90% of power.

You will need to report that sample size in Psych:EL. You will also need to upload the code with comments for:

1. The `function` that extracts the Bayes Factor.
2. The code you wrote for the simulation.
3. The code you wrote for calculating the power.

You will also need to create two plots and **upload it to Psych:EL as a PDF**:

1. A plot showing the distribution of the data you generated - `geom_density()`
2. A plot showing the power for each sample size - `geom_line()`

# Further Intermediate Reading

These readings will give you a general overview of this exact topic, but can
also serve as a good induction to a bit more thought-out power analysis.

- [Sch√∂nbrodt, F. D. & Wagenmakers, E.-J. (2018). Bayes Factor Design Analysis: Planning for compelling evidence. Psychonomic Bulletin & Review, 25, 128-142.](https://osf.io/d4dcu)
- [Vandekerckhove, J., Rouder, J. N., & Kruschke, J. K. (2018). Bayesian methods for advancing psychological science.](https://link.springer.com/article/10.3758/s13423-018-1443-8)

Here is a list of papers already using Bayesian Power Analysis:

- [BFDA in practice: A list of published examples](https://github.com/nicebread/BFDA/blob/master/BFDA_examples.md)

They can give you a few examples on how to write up what we have done for your
final reports.

## REFERENCES
