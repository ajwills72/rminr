---
title: "Creativity and the environment"
author: "Jon May, Andy Wills"
output: html_document
---
```{r setup, include=FALSE}
## DEVELOPERS: Uncomment one option, as appropriate

## Show only commands.
## knitr::opts_chunk$set(echo = TRUE, message = FALSE, results='hide', fig.keep = 'none', comment=NA)

## Show commands and ouptut.
knitr::opts_chunk$set(echo = TRUE, comment=NA)
```

## Before you start...
Before starting this exercise, you should have completed all the relevant [Absolute Beginners', Part 1](index.html) worksheets. Each section below indicates which of the earlier worksheets are relevant.

## Getting the data into R

**Relevant worksheet:** [Using RStudio projects](using-projects.html)

In this excercise, you'll be analysing some data that you and your peers recently collected. To get this data into R, follow these steps:

1. [Set up an RStudio project](using-projects.html) for this analysis. 

2. [Upload the CSV file](entering-data-by-hand.html#upload) you have been given for this activity into your RStudio project folder. If you want to try out this worksheet without that data file, you can use this [example CSV file](data/green.csv) instead. _You can only complete your PsycEL activity if you  use the CSV file you were sent_. 

3. Load the _tidyverse_ package, and then load your data into R.

```{r init-load, message=FALSE} 
library(tidyverse)
data <- read_csv("green.csv")
```

**Note**: In the example above, you'll need to replace `green.csv` with the name of the CSV file you just uploaded into your RStudio project.

### Inspect
Look at the data by clicking on it in the _Environment_ tab in RStudio. 

Each row is a rating by one participant in this study of creativity. Groups of participants came up with a creative solution to a problem, while either taking a walk in an _urban_ environment or a _nature_ environment. Each of these solutions has been rated for creativity by a set of raters. 

_Will the nature environment lead to more creative ideas than the urban environment?_

| Column    | Description                             | Values             |
| --------- | --------------------------------------- | ------------------ |
| Solution  | Number of the Solution | a number |
| Rater  | Reference number of the person rating the solution | a number |
| Cond      | Which environment was the creator in? | "Urban", "Nature" |
| score  | How creative was the idea rated to be? | 0-100, higher numbers = more creative |

## Pre-processing

**Relevant worksheet:** [Group Differences](group-differences.html)

We start by "pre-processing" our data, in order to make it easier to analyse. We do this in two steps:

### 1. Cleaning the data set

In some cases, a participant did not provide a rating of a solution -- this is then represented in the dataset as `NA`. R uses `NA` to specify that this data point is missing -- in this case, because the participant didn't respond.

Although it's good to explicitly record that a response was not made, keeping these `NA`  in the dataset will cause problems later on, so we're going to remove them:

```{r drop_na}
data  <- data %>% drop_na(score)
```

The command `drop_na(score)` is new  -- it just means remove the rows of the dataset where the score is recorded as `NA`. The rest of the command uses things we covered in the Group Differences worksheet -- the dataframe `data` is sent (i.e., piped, `%>%`) to the `drop_na()` command, which removes the `NA`, and the results are stored (`<-`) back in the `data` dataframe.

### 2. Summarising

Each solution was rated by several people. We're going to take the average (mean) of those ratings, so we're left with one creativity score per solution. 
We use the `group_by`,  `summarise`, and `mean` commands we used in the _Group Differences_ worksheet to do this: 

```{r preproc}
creative <- data %>% group_by(Cond, Solution) %>% summarise(score = mean(score))
```

As [before](group-differences.html#group), you can safely ignore the "ungrouping" message that you receive.

If we look at this summarised data, by clicking on the _Environment_ tab of RStudio, we can see that we now have one creativity score per solution.

## Creativity and the environment

**Relevant worksheets:** [Group Differences](group-differences.html), [Evidence](evidence.html)


We start by looking to see how the mean creativity scores differ for those who were in a nature or an urban environment. We can do this using the `group_by` and `summarise` functions in a similar way to before, but on our preprocessed data, which we have stored in the data frame `creative`:

```{r means}
creative %>% group_by(Cond) %>% summarise(mean(score))
```

Your output will look similar to this, but the numbers will probably be different. In this example, it looks like there's a small difference, with the creativity ratings slightly higher in the Nature environment -- but how does this between-group difference compare to the within-group variability? As we covered in the _Group Differences_ worksheet, this is most easily looked at with a scaled density plot:

```{r density}
creative %>% ggplot(aes(score, colour=factor(Cond))) + geom_density(aes(y=..scaled..)) + xlim(0, 100)
```

**Explanation of command:** The only new part here is `xlim(0, 100)`, which sets **lim**its on the **x**-axis of your graph. Specifically, it forces the lowest value on the x-axis to be 0 and the highest value to be 100. Without `xlim`, R chooses limits that it thinks are sensible. Like all computer programs, R isn't that bright, so often it makes sense to tell it more precisely what you want. 

In this example, the graph tells a somewhat different story to the means - although a difference between groups is visible, it is small compared to the variability within each group.

We can express the size of the difference in means, relative to the within-group variability, as an effect size. As we said in the _Group Differences_ worksheet, we calculate an effect size in R like this:

```{r effect}
library(effsize)
cohen.d(creative$score ~ creative$Cond)
```

In this example, the effect size is around 0.21, which is typically described as a small effect. The effect size for your data may be different.

At this point, the most pressing question is probably whether the difference observed in the mean scores is likely to be real, or whether it's more likely down to chance. As we saw in the _Evidence_ worksheet, the best way to look at this is with a Bayesian t-test:

```{r bf2,message=FALSE}
library(BayesFactor, quietly = TRUE)
ttestBF(formula = score ~ Cond, data = data.frame(creative))
```

The Bayes Factor in this case is approximately a 1/2 (0.41 to be more precise), meaning it's about twice as likely there _isn't_ a difference as there is. Your number will likely be a bit different.

**Enter the mean creativity score for each condition, the effect size, and the Bayes Factor for the difference, into PsycEL**.

Using the convention that there is a difference if BF > 3, there isn't a difference if BF < 0.33, and if it's between 0.33 and 3, we're unsure, select **difference, no difference, or unsure**, on PsycEL.

___

This material is distributed under a [Creative Commons](https://creativecommons.org/) licence. CC-BY-SA 4.0. 

