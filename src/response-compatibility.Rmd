---
title: "Response compatibility"
author: "Patric Bach, Chris Mitchell, Andy Wills"
output: html_document
---

```{r setup, include=FALSE}
## DEVELOPERS: Uncomment one option, as appropriate

## Show only commands.
## knitr::opts_chunk$set(echo = TRUE, message = FALSE, results='hide', fig.keep = 'none', comment=NA)

## Show commands and ouptut.
knitr::opts_chunk$set(echo = TRUE, comment=NA)
```

## Before you start...
Before starting this exercise, you should have completed **all** the [Absolute Beginners, Part 1](index.html) worksheets. If not, take a look at those exercises before continuing. Each section below also indicates which of the earlier worksheets are relevant.

## Getting the data into R
**Relevant worksheet:** [Intro to RStudio](intro-rstudio.html)

In order to complete this worksheet, you'll need to have downloaded your CSV file from the PsycEL exercise. **See the instructions on PsycEL for how to do this.**

Once you have downloaded your CSV file, [open a project](intro-rstudio.html) on RStudio Server for this analysis, create a script file, and [upload your CSV](entering-data-by-hand.html#upload) to your project.

**Plymouth University students**: Create/open your project named `psyc413`; within that create a script file called `resp-compat.R`. Enter all commands into that script and run them from there.

## Exploring your data

### Load
**Relevant worksheet:** [Exploring incomes](exploring-incomes.html) 

Load the _tidyverse_ package, and load your data.
```{r init-load, message=FALSE} 
library(tidyverse)
compdata <- read_csv("respcompat.csv")
```

### Inspect
Look at the data by clicking on it in the _Environment_ tab in RStudio. Here's what each of the columns in the data set contain:

| Column | Description                             | Values             |
| ------ | --------------------------------------- | ------------------ |
| trial  | Trial number                            | 1 - 96            |
| cond   | Condition (compatible vs. incompatible) | compat, incompat   |
| error  | Did you make an error?                             | 1 = Yes, 0 = No    |
| RT     | Reaction time                           | value in milliseconds |
| instruct | Key you were told to press            | "L" = left, "R" = right |
| seen     | Key you saw being pressed by another             | "L" = left, "R" = right |
| resp     | Response you made                      | "L" = left, "R" = right |


### Mean reaction time (RT)
**Relevant worksheet:** [Group differences](group-differences.html). 

Did you experience automatic imitation? If so, then your own responses should be faster if you had to make the same action as the hand on the screen, compared to a different action. 
To find this out, you can calculate the average (mean) response times for each of the two conditions: when your finger press was the same as the finger press on the screen (compatible responses) and when your finger press was different to the one on the screen (incompatible responses). You can then find out in which of the two conditions your response times were faster.
The `cond` column in your data shows the condition for each trial, while the `RT` column shows you how long you took (in milliseconds) to press the key in this trial.

To look at this, you will first need to make sure that you filter out the trials in which you made an error. People always behave a bit differently when they are making an error; these trials therefore need to be removed first. You can do this with the following command:

```{r filter}
cordata <- compdata %>% filter(error == 0)
```

Recall that the `filter` command needs to be told which data to _keep_, so we tell it to keep the data where `error` is set to zero, i.e. you were correct. This data is put into a new dataframe, called `cordata`.

Another problem is that response times measurements are sometimes strongly skewed by rare very slow responses, for example when people get distracted by their mobile ringing and only respond after several seconds. It is therefore good to remove such very long “outlier” trials -- this is much like what you did with outlier incomes in the [Exploring Incomes](exploring-incomes.html) worksheet. Usually people respond well within 2 seconds. Remove all trials with responses longer than that with the following command:

```{r filter-rt}
cordata <- cordata %>% filter(RT < 2000)
```

Recall that your data file measures reaction time in milliseconds, and that `filter` needs to be told what to **keep**, so you `filter` to keep RTs of less than 2000ms.

Now you are ready to calculate the mean reaction time for the two conditions of your experiment (_compatible_ and _incompatible_ trials), using this command:

```{r rt-means}
cordata %>% group_by(cond) %>% summarise(mean(RT))
```
The output will look something like the above (the actual numbers won't be the same). In the above example, the participant responded on average about 40 milliseconds more quickly in compatible compared to incompatible trials.

As [before](group-differences.html#group), you can safely ignore the "ungrouping" message that you receive.

**Enter the exact values that you found for the mean RTs in each condition into the PsycEL answer section**.

### Standard deviations of RT
**Relevant worksheet:** [Exploring incomes](exploring-incomes.html). 

It is also useful to look at the variability of your responses times in both conditions. Maybe you always responded with roughly the same speed when yours and the seen finger press were the same, but not when they were different. For example, it could be you only slowed down your responses in some of the incompatible trials. This would be reflected in larger standard deviation of the responses times in the incompatible compared to the compatible trials.
To get the standard deviations, you'll need to modify the command above by changing `mean` to `sd`. 

```{r rt-sd, echo=FALSE}
cordata %>% group_by(cond) %>% summarise(sd(RT))
```
If you get it right, the output will look something like this (the actual numbers won't be the same).

### RT histogram
**Relevant worksheet:** [Group differences](group-differences.html).

One way of looking at variability of response times graphically, as you covered in the [Group Differences](group-differences.html) worksheet, is to produce a diagram of the response time distributions in each condition. This will immediately show you whether your responses in both conditions have roughly the same shape and are just shifted earlier or later, or whether one of them has a longer "tail", for example, indicating a higher incidence of slower responses in particular. 

Take a look at the distribution of reaction times, using the following command. 

```{r rt-hist}
cordata %>% ggplot(aes(RT, colour=factor(cond))) + geom_density(aes(y=..scaled..)) 
```

If you get it right, the output will look something like the above (it won't be exactly the same). 

_Now do the following:_

1. Export your density plot, using the _Export_ icon on RStudio's _Plots_ window, and selecting "Save as image...". Give it a meaninful file name (e.g. "rt-dist") and click 'Save'. 

2. Download your density plot from RStudio server - see [these instructions](using-projects.html#download) for a reminder of how to do this.

3. **Upload your density plot** to PsycEL (see the PsychEL activity for instructions of how to do this).

### More on reaction times

You can use the results from the standard deviations and the RT histogram to diagnose why response times in the two conditions differ. There are several – not mutually exclusive – options. First, all responses could just be slightly slower in one condition (usually the incompatible trials) than in the other. Evidence for this would be that the standard deviations in both conditions are very similar and the RT histogram shows the same "shape"" in both conditions, with one just being slightly shifted rightwards. Alternatively, it could be that one condition is slower because people become more variable. If this is the case, then the standard deviations should be larger in one condition than the other, and the RT histogram should show a "flatter"" bulge that starts at the same point but extends further to the right. There is also a final, third, possibility. Most response times might be generally equally fast in both conditions, but one may show a higher incidence particularly of the slower responses. This would be evident in, again, a larger standard deviation in one condition. The RT histogram should show roughly overlapping peaks, but with a longer tail in one condition.
 
After having uploaded the RT histogram, **write a few sentences on the PsycEL page about why you think one condition is slower than the other**.

### Mean errors
**Relevant worksheet:** [Group differences](group-differences.html).

Finally, take a look at the mean errors by condition (compatible vs. incompatible). Typically, seeing a different action does not only make one slower, but also causes more errors, when people carry out the action that they saw rather than the action they would need to do in the given trials. To do this, you will need to work out the proportion of trials in which you made an error for each of the two conditions.

You'll need to use the `compdata` data fram for this, because the `cordata` dataframe only contains the trials on which you were correct. In the `compdata` dataframe, the `errors` column codes an error as `1` and a correct response as `0`. This is helpful, because the mean of this gives the probability of an error. Modify the command you used for mean reaction times to calculate the probability of error for each condition. 

```{r p-error, echo=FALSE}
compdata %>% group_by(cond) %>% summarise(mean(error))
```

If you get it right, the output will look something like this (the actual numbers won't be the same).

**Enter the exact values for the mean Error rates in each condition into the PsycEL answer section.**

___

This material is distributed under a [Creative Commons](https://creativecommons.org/) licence. CC-BY-SA 4.0. 

