---
author: Lenard Dome, Paul Sharpe, Andy Wills
title: Estimating Sample Size with Bayes Factor
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
date: '`r format(Sys.Date())`'
---

# Before you start

This is an advanced worksheet, which assumes you have completed the [Absolute Beginners' Guide to R](https://ajwills72.github.io/rminr/#beginners), the [Research Methods in Practice (Quantitative section)](https://benwhalley.github.io/rmip/overview-quantitative.html), and the [Intermediate Guide to R](https://ajwills72.github.io/rminr/#rmip).

If you understand the following worksheets, then you should be able to follow this one:

- [Evidence pt.1 Bayesian and traditional t-tests](https://ajwills72.github.io/rminr/vbg_evidence.html)
- [Evidence pt.2 Bayes and traditional correlation, scatterplot](https://ajwills72.github.io/rminr/vbg_corr.html)
- [Group Differences](https://www.andywills.info/rminr/group-differences.html)
- [Statistical Power](https://www.andywills.info/rminr/power.html)
- [More on Statistical Power](https://www.andywills.info/rminr/effsize_from_papers.html)

# What is the problem we are trying to solve?

One of the most important things in research is to make sure you have enough
evidence/data to make reliable conclusions. Collecting enough data requires
time and effort. In psychology, until recently, [researchers didn't collect
enough data](http://www.marjanbakker.eu/Bakker%20et%20al.%202016.pdf) to
make reliable conclusion with traditional frequentist statistics (methods that
rely on p-value significance testing).

Throughout your undergraduate course,
you relied on Bayesian techniques. One exemption was the
[worksheet](https://www.andywills.info/rminr/power.html), where you first
encountered the concept of statistical power. In that worksheet, you used
traditional frequentist techniques to estimate how many participants you need
to recruit for your experiment. The reason was that simply there is no
clear-cut formula for estimating sample size for and with Bayesian techniques.
Most techniques were beyond the requirements of an undergraduate course.
Fortunately, you now have the coding skill to tackle this problem with ease.
This method is also simple and require no sophisticated statistical and
mathematical skills.

# Brushing up on traditional power

In traditional null hypothesis significance testing, power analysis estimates
the likelihood of rejecting the null hypothesis given the effect size of the
alternative hypothesis. So if you have two groups, you could use a single effect
size, like that of [Cohen's *d*](https://www.andywills.info/rminr/power.html),
to find out how many participants you need to get a get a p-value below 0.05
with high probability. This is useful, because it explicitly states how likely
to achieve the researchers goal in the anticipated experiment
[(Kruschke, 2013)](https://jkkweb.sitehost.iu.edu/articles/Kruschke2013JEPG.pdf).
Ideally, you also want to do it before data collection
and not after. If you estimate power after the data is collected and analysed,
you will learn nothing that the Bayes Factor or a p-value wouldn't tell you.

Traditional techniques to estimate sample size are also limited. For example,
they don't allow you to plan for the null hypothesis being true. This is
because p-values don't let you conclude from null results.

For example, you might want to know how many participants you need to test so
that you will know for certain that two treatments (or a treatment and palcebo)
are no different. Another example is when there are contradicting evidence that
a psychological effect is real. You might want to know how many participants
you need to have to show that an effect is present in the data or not.

You can also have two estimates: one for the null hypothesis being true, and
one for the alternative being true. In that case, it is not unreasonable to
check the data at the lower estimate, to see if you need to collect more data.

A p-value and traditional techniques do not allow you to incorporate the
possibility that the null hypothesis is true, even though in some cases it might
be important to plan for that outcome as well.

Another important point to note is that effect sizes are uncertain
[(Kruschke, 2013)](https://jkkweb.sitehost.iu.edu/articles/Kruschke2013JEPG.pdf), but
traditional power calculations use a single point-value. Effect sizes can vary
from experiment from experiment (sometimes even substiantially), so it is
uncertain if you will get the same effect size with a new sample. This
experiment-to-experiment variance is not incorporated in traditional methods.
Effect sizes are also often [overestimated](http://marjanbakker.eu/Bakker%20Van%20Dijk%20Wicherts%202012.pdf)
which [causes a lot of problem](https://www.andywills.info/rminr/effsize_from_papers.html#replication) for scientist and researchers.

# Power analysis with Bayes Factors

In Bayesian Power Analysis, we are looking at power from a different
angle. Here, we want to estimate precision - the probability of the Bayes Factor
being conclusive or inconclusive. We determine this by checking whether the
Bayes Factor falls within a certain interval. Intervals are groups of numbers
lying between two numbers. This probability tells us how likely that we will
have conclusive results given different sample sizes.

In psychology, people like to think categorically, therefore [define
categories of BFs](https://www.andywills.info/rminr/evidence.html#bf)
depending on different intervals. Traditionally, a BF of three and above is
considered as evidence for a difference, and ⅓ and below is considered as
evidence for the absence of a difference.

This might be a bit confusing, because now we will be looking at intervals
and not thresholds, because we are interested in an entire distribution of
values. For
estimating the sample size, we can define two big groups of Bayes Factors:
conclusive vs inconclusive. Inconclusive BF can be any BF that falls between
the lower bound and the upper bound. All other BFs are conclusive.
We calculate power for the Bayes Factor (BF) as

$$1 - Pr(lower bound < BF < upper bound)$$

**When we think about Bayes Factors and Power, we want to know how
many data points we need to get a conclusive Bayes Factor.**

## The Algorithm

> An algorithm is a list of rules to follow in order to solve a problem.

The algorithm for estimating sample size for more sophisticated Bayesian
analysis was first described by Kruschke (2013) while he was promoting an
alternative to frequentist statistics.
[Schönbrodt and Wagenmakers (2018)](https://www.researchgate.net/publication/314158724_Bayes_factor_design_analysis_Planning_for_compelling_evidence) adapts
his algorithm to **fixed-n designs** with **Bayes Factor**. In a fixed-n
design, you test a set number of participants and calculate something like a
Bayes Factor at the end. Schönbrodt and Wagenmakers (2018) decided to group
this technique under an umbrella term: **Bayes Factor Design Analysis**.

We will use Schönbrodt and Wagenmakers (2018) algorithm, because it is the
simplest form and is suitable for most psychology experiments you will be doing
in your final year project.

Here I will show you the algorithm and then walk you through it. The steps
in the algorithm:

1. **Generate a large number (e.g. 2000) of random data points** by taking some assumptions about what to expect. Data should be generated according to how the actual data will be collected in the planned experiment. For example, there will be a fixed number of participants, fixed number of trials, fixed number of conditions. You also want to take some other assumptions, like what is the effect size you might expect.
2. **Subset the data to include a fixed number of data points** (40 participant). You ideally want to do it for different sample sizes for the same data set.
3. **Run a test on the subset** and tally whether the test is precise enough. You simply run a Bayes Factor t-test in the subsets and check if the Bayes Factor is between 1/3 and 3.
4. **Repeat steps 2 and 3 many thousands of times**, for example 10000.
5. **Pick** a well-powered sample size.

## Walk-through Bayesian Power Analysis

In order to estimate a sample size, we need to think about
both our experiment, our planned analysis and what other researchers have
already learned about the topic of interest.

There is a good chance that someone has done an experiment similar to what you
are planning. So you can use their data to find the effect size and estimate
something about your experiment - usually sample size. If you need it for
your own project, look at [this material](https://www.andywills.info/rminr/effsize_from_papers.html#estimate).

Let us say that we are interested in multistable perception and want to do
a replication of [McDougall and Smith (1921)](https://onlinelibrary-wiley-com.plymouth.idm.oclc.org/doi/pdfdirect/10.1002/j.2050-0416.1921.tb02471.x). They did not include the data, but
told us some details about their analysis, including some summary statistics.

McDougall and Smith (1921) were interested in how different substances (coffee,
antidepressant) affects the perception of a subset of visual illusions.
Here we will only focus on the condition where participants viewed an
an ambiguous figure called the
[Necker Cube](https://en.wikipedia.org/wiki/Necker_cube). A necker cube
is the frame of the cube with no visual cues on its orientation, which
makes it ambiguous. It is ambiguous, because while viewing the figure
you can interpret it to have either the lower-left or the upper-right square
at the front. If you keep looking at the cube, you can see that the orientiation
switches between these two percepts - having the lower-left or upper-right
square as its front side.

![Necker Cube](https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/Necker_cube_and_impossible_cube.svg/1280px-Necker_cube_and_impossible_cube.svg.png)

McDougall and Smith (1921) reported that people passively switch
between the two percepts
at a higher rate after they had coffee compared to when they hadn't had
any other drug.

### Starting with the data

```{r, include = FALSE}
# always start with loading all the packages you need for a given analysis
library(BayesFactor, quietly = TRUE)
library(effsize)
library(tidyverse)

# set seed for reproducability
set.seed(1)
```

```r
# always start with loading all the packages you need for a given analysis
library(BayesFactor, quietly = TRUE)
library(effsize)
library(tidyverse)

# set seed for reproducability
set.seed(1)
```

In most scenarios, you will start from the next step, but your supervisor might
have some data you can use. Nowadays people share their data on places like
[OSF](https://osf.io/) or [GitHub](https://github.com/). Here, I just made up
the data according to some really vague description by the original authors.
Let us start by importing said data.

You can download the data from this [link](https://raw.githubusercontent.com/ajwills72/rminr/power-bayes/src/data/mcdougall1921madeup.csv).

```{r}
# import dataset
dta <- read_csv("data/mcdougall1921madeup.csv")
```

Let us have a look at how the data look like.

```{r}
dta %>%
    group_by(condition) %>%
    summarise(mean = mean(fluctuations), sd = sd(fluctuations))
```

The means are indeed promising, but we have long learned not to base
our conclusion solely on the mean, so we go on and do the analysis we plan to
do on our own data.

```{r}
bf <- ttestBF(x = dta$fluctuations[dta$condition == "before"],
              y = dta$fluctuations[dta$condition == "after"], paired = TRUE)
bf
```

This is an inconclusive result. Some would say that it is
anecdotal evidence for the difference, but that is a bit of a cop-out.
We just don't have enough data to conclude.

Our results are not conclusive, as BF = `r data.frame(bf)$bf`. The next step
is then to figure out how many participants we need to recruit so that our
analysis can conclude. First, let us look at the effect size.

```{r}
cohen.d(fluctuations ~ condition | Subject(ppt), data = dta, paired = TRUE)
```

### Generating data

The next step is to come up with some generated data. If you don't have access
to any real data to begin with, you should start with this step. We know that
we have a small effect size with Cohen's $d = 0.217$ and that our inconclsive
BF shifts towards the alternative hypothesis - that there is a difference.
If the effect size is not reported by the authors, you can still work it out.
More information on how to do it can be found in the
[more on power worksheet](https://www.andywills.info/rminr/effsize_from_papers.html).

Based on these, we can quantitatively express what we expect to happen in
our experiment.
We do this by generating some data that will look exactly like the one
that we will collect. The first thing to jot down is to decide how our
dependent variable will look like.

We know that we will have count data, because
our dependent variable is the count of how many switches participants made
between having the lower-left or upper-right square as the cube's front side.
`rbinom` is the command that is most suitable to generating this type of
data. `rbinom` looks like this:

```r
rbinom(n, size, prob)
```

The first argument `n` will be the number of observation we
make (how many participants we want overall). We will set it to 1000,
because we will randomly pick a smaller subset from this 1000 when we
start estimating the sample size.

The second argument will be `size`,
which specifies the maximum number of fluctuation a person can experience
in a given time. We will set it to 100. We assume that participant will
fixate on the Necker Cube for 100 seconds and they can't have more than one
in a second. This is less important here, because `n` will scale up to
whatever `size` we set. Note that `size` has to be same for both conditions.

The third argument `prob` will determine the mean proportion of fluctuations
relative to fluctuating every second. This is that we will vary between conditions.
there will be a 3% increase in the experimental conditions, which will result
in a small effect size. We take the assumption that this effect size is the
mean effect size of this phenomenon. So if you have ran dozens of experiment,
all effect sizes would cluster around this value.

We will do the same for the experimental condition, but the mean proportion
will be larger. We pick the difference to reflect the effect size we saw in the
real data. Alternatively, if you have no data you can just use an effect size
you think is reasonable.

```{r}
# each data point is a single participant
control <- rbinom(1000, 100, 0.60)
experimental <- rbinom(1000, 100, 0.63)
# here we create a vector for the group column
group <- rep(c("before", "after"), each = 1000)
# this object will have the participant unique IDs
participants <- rep(1:1000, 2)
# here we combine everything together
ideal_data <- tibble(ppt = participants,
                     condition = group,
                     fluctuations = c(control, experimental))
```

Now we check whether it looks okay. First, we should have three columns.
`ppt` should include the participant ID, `condition` should include
whether it is before or after caffeine intake, and `fluctuations`
should be the number of fluctuations.

The distribution of the data also looks exactly what we want.

```{r}
ideal_data %>% ggplot(aes(x = fluctuations, fill = condition)) +
    geom_density(alpha = 0.5) +
    theme_void()
```

### Writing a function to get the Bayes Factor

Every function (commands) that you use has been written by a person. Functions
as you have noticed, take something as an input, do something to that input,
and return the result of what they have done to the input. There is some good
material already on [function](https://www.andywills.info/rminr/awdiss.html#funcloop)
by Paul Sharpe and Andy Wills. To put it briefly, a **function is an
instruction sequence that takes something as an input and translates it
into an output.**

Here we will write our own function that extracts the Bayes Factor for our
sample size. The function will need to know `n` which is our sample size,
and `data` which is the generated data.
So what does our function need to do? It needs to :

1. Take `n` number of randomly selected participant from `data`.
2. Then run the paired-sample Bayesian t-test on the subset data.
3. Store the output and put it into a format, where we can extract the Bayes Factor.
4. At the end, return the Bayes Factor.

```{r}
## we take the sample size n and a data set as inputs
get_bf <- function(n, data) {
    ## 1. we randomly pick n number of participants from data
    ideal_dta <- data %>% filter(ppt %in% sample(1000, n))
    ## 2. do the Bayesian t-test
    test <- ttestBF(x = ideal_dta$fluctuations[ideal_dta$condition == "before"],
                  y = ideal_dta$fluctuations[ideal_dta$condition == "after"],
                  paired = TRUE)
    ## 3. get the Bayes Factor
    ## we have to change the data format to extract the Bayes Factor
    test <- data.frame(test)
    ## copy it into a new R object
    bf <-  test$bf
    ## 4. return it to the user
    return(bf)
}
```

### Determining Sample Size

> Simulations are pretend games. When we say simulate, we mean to imitate some
real-world process over time.

Now we are at the stage when we can actually run a simulation. These simulations
will be based on a mathematical technique called random sampling or Monte Carlo
methods. Beware that there are no special equations or complicated math
involved here. Monte Carlo is simply a tool that is used to estimate possible
outcomes of uncertain events. In our case, the possible outcome is the Bayes
factor and the uncertain event is the data we collect. So we imitate
the whole scientific process by generating some random data, checking the Bayes
Factor for the data, and repeat it many times to get a robust and more accurate
estimate.

We have to repeat it many times, because the process of sampling *n* number
of data points is semi-random (not unlike how we should recruit participants).
The more you repeat it and the more data points you have, the more your
distribution will resemble to a normal distribution due to the
[central limit theorem](https://ajwills72.github.io/critical-thinking/distributions-samples.pdf). Monte Carlo methods also address another point
we made before: uncertainty in effect sizes. Because we sample randomly from
a much larger distribution of values, the difference will vary slightly every
time we draw *n* number of data points. The mean effect size remains the same,
but it allows us to make more precise and reliable estimate on how many
participants we need to recruit.

There are two things to decide before we start imitating data collection:

1. The sample sizes we want to check. We create all sample sizes as a `tibble`
object. This means that we will have a one-column data frame with all the
sample sizes we want to check.
2. The number of draws we wish to make for each sample sizes. This will simply
be the number of times a certain sample size occurs in our `tibble` data frame.


First, we will create sample sizes from 10 to 200. We will also repeat each sample size 1000 times.

Then we will mutate the data frame, which will add another column that we
call `bf`. We then use `map_dbl`, which can go over each sample size in our
`tibble` and run the `get_bf` function we just wrote. `get_bf` needs to
have the data as well, which we will add within the `map_dbl` command below.

```{r}
## this can take quite a while
monte <- tibble(n = rep(seq(10, 200, 10), each = 1000)) %>%
    mutate(bf = map_dbl(n, get_bf, data = ideal_data))
```

Now we have everything that we need. So let us do the math.

1. We will group by the sample sizes.

2. Check whether the BF fell between 1/3 and 3. We do this with the
command `between`, which will return TRUE if the value of interest falls
between the lower and upper bounds of the interval. So it will look like
`between(value, lower, upper)`. For example, `between(30, 18, 60)`, would
return TRUE, because 30 is somewhere between 18 and 60.

3. We then `sum` it up, which will count the number of times it returns TRUE.
Divide that number by a 100 to get the proportions.

4. Then we move on to subtract it from 1, which will give us the proportion of
times we got a Bayes Factor that was conclusive - outside of the interval
1/3 and 1.

```{r}
monte %>%
    group_by(n) %>%
    summarise(power = 1 - (sum(between(bf, 1/3, 3)) / 1000)) %>%
    data.frame()
```

The only thing that remains is to find out the actual sample size we need.
In frequentist methods, people usually expect your study to have at least
80% power. If we accept that convention for this as well, **we can settle on
a sample size of 50.** On the other hand, the story doesn't end here.

This is the exact moment, when you might want to stop and think about how
many participants you might potentially end up excluding. This depends a lot
on the nature of your experiment and the characteristics of the population.
If you have a learning criterion, you can end up excluding 30% of all
participants. It is not uncommon to add 15 more participants on top of that 50,
just to be safe.

Before moving on to the exercises, it is worth looking at the how power improves
for each sample size.

As you can see, the power goes up not linearly, but more as
a curve. This resembles more to a logarithmic function, which has this
steep curve that seems to plateau after a certain point. Power after a certain
point remains stationary. This means that collecting too much data is
not harmful, but can be unnecessary.

```{r}
monte %>%
    group_by(n) %>%
    summarise(power = 1 - (sum(between(bf, 1/3, 3)) / 1000)) %>%
    mutate(criterion = power > 0.80) %>%
    ggplot(aes(x = n, y = power)) +
    geom_line(linetype = 2) +
    geom_point(aes(colour = criterion), size = 2) +
    geom_vline(aes(xintercept = 50)) +
    scale_x_continuous(breaks = sort(c(50, seq(0, 200, 100)))) +
    xlab("sample size") +
    ylab("power") +
    theme_classic()
```

You can see on this graph, that beyond a certain sample size, there are no
improvement. This means that keep collecting data will add no detectable
benefit to your analysis.

# Exercise 1: Estimate the sample for a null effect

In this exercise you will need to estimate the sample size for detecting
null effect. Additionally, you will need to do so for a harsher interval,
where the Bayes Factor falls outside of 1/10 and 10. You will need
to find the sample size, that will give you at least 80% of power.

On Psyc:EL, you will need to insert the code you used to estimate the sample
size for the null hypothesis.

Hint: the null hypothesis essentially means that there is no difference between
conditions, so the means are equal. You might also need to increase how many
sample sizes you need to test.

# Exercise 2: Create a plot.

**This is a Psyc:EL task.**

Using the estimate from Exercise 1, create a plot showing power for each sample
size and **upload it to Psych:EL as a PDF**. You will need to edit the code for
the plot we have already created once, so you already have a template to go by.
Remember to (a) add the line for the adequate sample size and (b) edit the axis
ticks, so it includes the lowest estimated sample size with at least 80% power.

# Further Intermediate Reading

These readings will give you a general overview of this exact topic, but can
also serve as a good induction to a bit more thought-out power analysis.

- [Schönbrodt, F. D. & Wagenmakers, E.-J. (2018). Bayes Factor Design Analysis: Planning for compelling evidence. Psychonomic Bulletin & Review, 25, 128-142.](https://link.springer.com/content/pdf/10.3758/s13423-017-1230-y.pdf)
- [Vandekerckhove, J., Rouder, J. N., & Kruschke, J. K. (2018). Bayesian methods for advancing psychological science.](https://link.springer.com/article/10.3758/s13423-018-1443-8)

Here is a list of papers already using Bayesian Power Analysis:

- [BFDA in practice: A list of published examples](https://github.com/nicebread/BFDA/blob/master/BFDA_examples.md)

They can give you a few examples on how to write up what we have done for your
a potential reports.

___

This material is distributed under a [Creative Commons](https://creativecommons.org/) licence. CC-BY-SA 4.0.
